{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a114b7b5-58cd-4a68-b82c-766a76667c46",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d048c-bed0-4f82-a9b3-e4ea7d51e806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "!pip install category_encoders\n",
    "!pip install citylearn==2.1b9\n",
    "\n",
    "\n",
    "# Because of the Wrapper Error\n",
    "!pip install keras==2.12.0\n",
    "!pip install tensorflow==2.12.0\n",
    "\n",
    "# Then you have to reinitiate the kernel! and install scikit-optimize!\n",
    "!pip install scikit-optimize\n",
    "!pip install sktime\n",
    "!pip install skforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4071c164-b5b6-4eae-910f-081322985fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 07:14:02.700972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from my_models.user_model import SubmissionModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import gp_minimize\n",
    "from functools import partial\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sktime.forecasting.compose import (TransformedTargetForecaster,\n",
    "                                        make_reduction)\n",
    "\n",
    "import skforecast as skforecast\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5489a18-4f56-45ac-b34d-2cf9bf6bb33f",
   "metadata": {},
   "source": [
    "## Simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05333283-0a9d-4ca3-ae1c-6ce1378e18a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a test env\n",
    "class WrapperEnv:\n",
    "    \"\"\"\n",
    "    Env to wrap provide Citylearn Env data without providing full env\n",
    "    Preventing attribute access outside of the available functions\n",
    "    \"\"\"\n",
    "    def __init__(self, env_data):\n",
    "        self.observation_names = env_data['observation_names']\n",
    "        self.action_names = env_data['action_names']\n",
    "        self.observation_space = env_data['observation_space']\n",
    "        self.action_space = env_data['action_space']\n",
    "        self.time_steps = env_data['time_steps']\n",
    "        self.seconds_per_time_step = env_data['seconds_per_time_step']\n",
    "        self.random_seed = env_data['random_seed']\n",
    "        self.buildings_metadata = env_data['buildings_metadata']\n",
    "        self.episode_tracker = env_data['episode_tracker']\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        return {'buildings': self.buildings_metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0049ca9f-53b0-417c-9f3b-13293825d09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_citylearn_env(config):\n",
    "    env = CityLearnEnv(config.SCHEMA)\n",
    "\n",
    "    env_data = dict(\n",
    "        observation_names = env.observation_names,\n",
    "        action_names = env.action_names,\n",
    "        observation_space = env.observation_space,\n",
    "        action_space = env.action_space,\n",
    "        time_steps = 720,\n",
    "        buildings_metadata = env.get_metadata()['buildings'],\n",
    "        num_buildings = len(env.buildings),\n",
    "        building_names = [b.name for b in env.buildings],\n",
    "        b0_pv_capacity = env.buildings[0].pv.nominal_power,\n",
    "    )\n",
    "\n",
    "    # Turn off actions for all buildings and do not simulate power outage (forecasting only).\n",
    "    for b in env.buildings:\n",
    "        b.ignore_dynamics = True\n",
    "        b.simulate_power_outage = False\n",
    "\n",
    "    return env, env_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe70e10-5cce-4035-8bd9-7486201d83b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    data_dir = './data/'\n",
    "    SCHEMA = os.path.join(data_dir, 'schemas/warm_up/schema.json')\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf5221e-57fb-44ee-85b0-1fa6605e63b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env, env_data = create_citylearn_env(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15054c39-c5e2-477f-93fa-0a22f69b6242",
   "metadata": {},
   "source": [
    "## Generation of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1783afd-efc0-4205-aa31-3bda2673da08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "\n",
    "b_1_dataframe = pd.DataFrame(columns=['day_type', 'hour', 'outdoor_dry_bulb_temperature', 'outdoor_dry_bulb_temperature_predicted_6h', \n",
    "                                      'outdoor_dry_bulb_temperature_predicted_12h', 'outdoor_dry_bulb_temperature_predicted_24h', 'diffuse_solar_irradiance',\n",
    "                                      'diffuse_solar_irradiance_predicted_6h', 'diffuse_solar_irradiance_predicted_12h', \n",
    "                                      'diffuse_solar_irradiance_predicted_24h', 'direct_solar_irradiance', 'direct_solar_irradiance_predicted_6h',\n",
    "                                      'direct_solar_irradiance_predicted_12h', 'direct_solar_irradiance_predicted_24h', 'carbon_intensity', \n",
    "                                      'indoor_dry_bulb_temperature', 'non_shiftable_load', 'solar_generation', 'dhw_storage_soc', 'electrical_storage_soc', \n",
    "                                      'electricity_pricing', 'electricity_pricing_predicted_6h', \n",
    "                                      'electricity_pricing_predicted_12h', 'electricity_pricing_predicted_24h', 'cooling_demand',\n",
    "                                      'dhw_demand','indoor_dry_bulb_temperature_set_point','occupant_count','net_electricity_consumption'])\n",
    "\n",
    "b_2_dataframe = pd.DataFrame(columns=['day_type', 'hour', 'outdoor_dry_bulb_temperature', 'outdoor_dry_bulb_temperature_predicted_6h', \n",
    "                                      'outdoor_dry_bulb_temperature_predicted_12h', 'outdoor_dry_bulb_temperature_predicted_24h', 'diffuse_solar_irradiance',\n",
    "                                      'diffuse_solar_irradiance_predicted_6h', 'diffuse_solar_irradiance_predicted_12h', \n",
    "                                      'diffuse_solar_irradiance_predicted_24h', 'direct_solar_irradiance', 'direct_solar_irradiance_predicted_6h',\n",
    "                                      'direct_solar_irradiance_predicted_12h', 'direct_solar_irradiance_predicted_24h', 'carbon_intensity', \n",
    "                                      'indoor_dry_bulb_temperature', 'non_shiftable_load', 'solar_generation', 'dhw_storage_soc', 'electrical_storage_soc', \n",
    "                                      'electricity_pricing', 'electricity_pricing_predicted_6h', \n",
    "                                      'electricity_pricing_predicted_12h', 'electricity_pricing_predicted_24h', 'cooling_demand',\n",
    "                                      'dhw_demand','indoor_dry_bulb_temperature_set_point','occupant_count','net_electricity_consumption'])\n",
    "\n",
    "b_3_dataframe = pd.DataFrame(columns=['day_type', 'hour', 'outdoor_dry_bulb_temperature', 'outdoor_dry_bulb_temperature_predicted_6h', \n",
    "                                      'outdoor_dry_bulb_temperature_predicted_12h', 'outdoor_dry_bulb_temperature_predicted_24h', 'diffuse_solar_irradiance',\n",
    "                                      'diffuse_solar_irradiance_predicted_6h', 'diffuse_solar_irradiance_predicted_12h', \n",
    "                                      'diffuse_solar_irradiance_predicted_24h', 'direct_solar_irradiance', 'direct_solar_irradiance_predicted_6h',\n",
    "                                      'direct_solar_irradiance_predicted_12h', 'direct_solar_irradiance_predicted_24h', 'carbon_intensity', \n",
    "                                      'indoor_dry_bulb_temperature', 'non_shiftable_load', 'solar_generation', 'dhw_storage_soc', 'electrical_storage_soc', \n",
    "                                      'electricity_pricing', 'electricity_pricing_predicted_6h', \n",
    "                                      'electricity_pricing_predicted_12h', 'electricity_pricing_predicted_24h', 'cooling_demand',\n",
    "                                      'dhw_demand','indoor_dry_bulb_temperature_set_point','occupant_count','net_electricity_consumption'])\n",
    "\n",
    "b_dataframe_list = [b_1_dataframe,b_2_dataframe,b_3_dataframe]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c42120-ce5a-46d1-af63-a279e2425823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.67788136], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, b in enumerate(env.buildings):\n",
    "    \n",
    "    con = b.net_electricity_consumption\n",
    "    print(str(len(con)))\n",
    "    \n",
    "env.buildings[0].net_electricity_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db48ddf-2417-4ca8-9663-70c05a33e7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the Datasets for the different buildings:\n",
    "# Here I only need to simulate the ones, which are not present in the dataset:\n",
    "\n",
    "for idx, b in enumerate(env.buildings):\n",
    "    indoor_dry_bulb_temperature           = b.energy_simulation.indoor_dry_bulb_temperature\n",
    "    non_shiftable_load                    = b.energy_simulation.non_shiftable_load\n",
    "    solar_generation                      = b.energy_simulation.solar_generation\n",
    "    dhw_storage_soc                       = b.dhw_storage.soc\n",
    "    electrical_storage_soc                = b.electrical_storage.soc\n",
    "    cooling_demand                        = b.energy_simulation.cooling_demand\n",
    "    dhw_demand                            = b.energy_simulation.dhw_demand\n",
    "    indoor_dry_bulb_temperature_set_point = b.energy_simulation.indoor_dry_bulb_temperature_set_point\n",
    "    occupant_count                        = b.occupant_count.repeat(720)\n",
    "    net_electricity_consumption           = b.net_electricity_consumption.repeat(720)\n",
    "    \n",
    "    # After the generation of the different features I will add the global features (which are independend from the houses!)\n",
    "    day_type         = env.buildings[0].energy_simulation.day_type\n",
    "    hour             = env.buildings[0].energy_simulation.hour\n",
    "    carbon_intensity = env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "    # Loading the local features\n",
    "    filepath = 'data/schemas/warm_up/'\n",
    "\n",
    "    pricing    = pd.read_csv(filepath + 'pricing.csv')\n",
    "    weather    = pd.read_csv(filepath + 'weather.csv')\n",
    "\n",
    "    electricity_pricing                = pricing['Electricity Pricing [$/kWh]']\n",
    "    electricity_pricing_predicted_6h   = pricing['6h Prediction Electricity Pricing [$/kWh]']\n",
    "    electricity_pricing_predicted_12h  = pricing['12h Prediction Electricity Pricing [$/kWh]']\n",
    "    electricity_pricing_predicted_24h  = pricing['24h Prediction Electricity Pricing [$/kWh]']\n",
    "\n",
    "    outdoor_dry_bulb_temperature                = weather['Outdoor Drybulb Temperature (C)']\n",
    "    outdoor_dry_bulb_temperature_predicted_6h   = weather['6h Outdoor Drybulb Temperature (C)']\n",
    "    outdoor_dry_bulb_temperature_predicted_12h  = weather['12h Outdoor Drybulb Temperature (C)']\n",
    "    outdoor_dry_bulb_temperature_predicted_24h  = weather['24h Outdoor Drybulb Temperature (C)']\n",
    "\n",
    "    diffuse_solar_irradiance                    = weather['Diffuse Solar Radiation (W/m2)']\n",
    "    diffuse_solar_irradiance_predicted_6h       = weather['6h Diffuse Solar Radiation (W/m2)']\n",
    "    diffuse_solar_irradiance_predicted_12h      = weather['12h Diffuse Solar Radiation (W/m2)']\n",
    "    diffuse_solar_irradiance_predicted_24h      = weather['24h Diffuse Solar Radiation (W/m2)']\n",
    "\n",
    "    direct_solar_irradiance                     = weather['Direct Solar Radiation (W/m2)']\n",
    "    direct_solar_irradiance_predicted_6h        = weather['6h Direct Solar Radiation (W/m2)']\n",
    "    direct_solar_irradiance_predicted_12h       = weather['12h Direct Solar Radiation (W/m2)']\n",
    "    direct_solar_irradiance_predicted_24h       = weather['24h Direct Solar Radiation (W/m2)']\n",
    "    \n",
    "    # Generate the Dataframe for the training\n",
    "    b_dataframe_list[idx]['day_type']                                   = day_type\n",
    "    b_dataframe_list[idx]['hour']                                       = hour\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature']               = outdoor_dry_bulb_temperature\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature_predicted_6h']  = outdoor_dry_bulb_temperature_predicted_6h\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature_predicted_12h'] = outdoor_dry_bulb_temperature_predicted_12h\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature_predicted_24h'] = outdoor_dry_bulb_temperature_predicted_24h\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance']                   = diffuse_solar_irradiance\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance_predicted_6h']      = diffuse_solar_irradiance_predicted_6h\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance_predicted_12h']     = diffuse_solar_irradiance_predicted_12h\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance_predicted_24h']     = diffuse_solar_irradiance_predicted_24h\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance']                    = direct_solar_irradiance\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance_predicted_6h']       = direct_solar_irradiance_predicted_6h\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance_predicted_12h']      = direct_solar_irradiance_predicted_12h\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance_predicted_24h']      = direct_solar_irradiance_predicted_24h\n",
    "    b_dataframe_list[idx]['carbon_intensity']                           = carbon_intensity\n",
    "    b_dataframe_list[idx]['indoor_dry_bulb_temperature']                = indoor_dry_bulb_temperature\n",
    "    b_dataframe_list[idx]['non_shiftable_load']                         = non_shiftable_load\n",
    "    b_dataframe_list[idx]['solar_generation']                           = solar_generation\n",
    "    b_dataframe_list[idx]['dhw_storage_soc']                            = dhw_storage_soc\n",
    "    b_dataframe_list[idx]['electrical_storage_soc']                     = electrical_storage_soc\n",
    "    b_dataframe_list[idx]['electricity_pricing']                        = electricity_pricing\n",
    "    b_dataframe_list[idx]['electricity_pricing_predicted_6h']           = electricity_pricing_predicted_6h\n",
    "    b_dataframe_list[idx]['electricity_pricing_predicted_12h']          = electricity_pricing_predicted_12h\n",
    "    b_dataframe_list[idx]['electricity_pricing_predicted_24h']          = electricity_pricing_predicted_24h\n",
    "    b_dataframe_list[idx]['cooling_demand']                             = cooling_demand\n",
    "    b_dataframe_list[idx]['dhw_demand']                                 = dhw_demand\n",
    "    b_dataframe_list[idx]['indoor_dry_bulb_temperature_set_point']      = indoor_dry_bulb_temperature_set_point\n",
    "    b_dataframe_list[idx]['occupant_count']                             = occupant_count\n",
    "    b_dataframe_list[idx]['net_electricity_consumption']                = net_electricity_consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36edbc-8963-4cf9-853e-c087c493193c",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402aa4b-066a-4024-806a-f07878d5f764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the important features into files\n",
    "b = 1\n",
    "for data in b_dataframe_list:\n",
    "    feature_selection(data,'cooling_demand')\n",
    "    feature_selection(data,'dhw_demand')\n",
    "    feature_selection(data,'non_shiftable_load')\n",
    "    feature_selection(data,'carbon_intensity')\n",
    "    feature_selection(data,'solar_generation')\n",
    "    b = b + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc97c1a-68e1-4d74-a6e1-301ff68e91a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_selection(data,obs_feature):\n",
    "    # Split the dataset into features and target\n",
    "    X = data\n",
    "    y = data[obs_feature]\n",
    "    \n",
    "    # Apply Information Gain\n",
    "    ig = mutual_info_regression(X, y)\n",
    "\n",
    "    # Create a dictionary of feature importance scores\n",
    "    feature_scores = {}\n",
    "    i = 0\n",
    "    for (columnName, columnData) in data.items():\n",
    "        feature_scores[columnName] = ig[i]\n",
    "        i = i + 1\n",
    "    # Sort the features by importance score in descending order\n",
    "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    f_l = []\n",
    "    s_l = []\n",
    "    a_l = []\n",
    "    a_l_s = []\n",
    "    # Print the feature importance scores and the sorted features\n",
    "    for feature, score in sorted_features:\n",
    "        a_l.append(feature)\n",
    "        a_l_s.append(score)\n",
    "        if score > 0.10:\n",
    "            # save the features\n",
    "            f_l.append(feature)\n",
    "            s_l.append(score)\n",
    "            \n",
    "    dic = {'feature': f_l, 'score': s_l}\n",
    "    dic_a = {'feature': a_l, 'score': a_l_s}\n",
    "    df2 = pd.DataFrame(dic_a)\n",
    "    df = pd.DataFrame(dic)\n",
    "    df.to_csv('data/features/feature_importance_'+str(obs_feature)+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84e00c-1958-4f58-9c60-8de58a0e39e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Predictors (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b41fd673-11e2-4f20-a660-fb36934ccd2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'lstm'\n",
    "hyperparameter = False\n",
    "\n",
    "if 'BahdanauAttention' not in tf.keras.utils.get_custom_objects():\n",
    "    register_keras_serializable('BahdanauAttention')(BahdanauAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0382c-bd19-474a-b753-fa3e54dbeb9f",
   "metadata": {},
   "source": [
    "## Building Level Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a8265-0e7c-4c65-836c-6bb7a55701e1",
   "metadata": {},
   "source": [
    "### 1.) Cooling Load (kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da7752-a409-4bf3-a5ed-fc0d82380079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "for b in b_dataframe_list:\n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'cooling_demand')\n",
    "        joblib.dump(xgb, 'my_models/models/cooling_demand_model_b'+str(i)+'_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        lgb = MultiStepLightGBM(b,hyperparameter,'cooling_demand')\n",
    "        skforecast.utils.save_forecaster(lgb,'my_models/models/LightGBM/cooling_demand_model_b'+str(i)+'.h5', verbose=False)\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'cooling_demand')\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'cooling_demand')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/cooling_demand_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/cooling_demand_model_b'+str(i)+'.pkl')\n",
    "\n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Bi_Model(b,hyperparameter,'cooling_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_BiAttention/cooling_demand_model_b'+str(i)+'_hyper.h5')\n",
    "        \n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'cooling_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/cooling_demand_model_b'+str(i)+'_hyper.h5')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5d417-bec7-49b8-8f1a-2b30b199bbf1",
   "metadata": {},
   "source": [
    "### 2.) DHW Load (kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2291829-e34d-491d-a21f-9f6b8c981d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "for b in b_dataframe_list:\n",
    "\n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'dhw_demand')\n",
    "        joblib.dump(xgb, 'my_models/models/dhw_demand_model_b'+str(i)+'_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        lgb = MultiStepLightGBM(b,hyperparameter,'dhw_demand')\n",
    "        skforecast.utils.save_forecaster(lgb, 'my_models/models/LightGBM/dhw_demand_model_b'+str(i)+'.h5', verbose=False)\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'dhw_demand')\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'dhw_demand')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/dhw_demand_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/dhw_demand_model_b'+str(i)+'.pkl')\n",
    "        \n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Bi_Model(b,hyperparameter,'dhw_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_BiAttention/dhw_demand_model_b'+str(i)+'_hyper.h5')\n",
    "                        \n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'dhw_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/dhw_demand_model_b'+str(i)+'_hyper.h5')\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f10a61-a65d-4a70-ad5f-01f02f0a2049",
   "metadata": {},
   "source": [
    "### 3.) Equipment Electric Power (kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ecb764-a3e2-4266-a1ae-1da596ce021f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4731\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4697\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.7968\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5412\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5297\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5730\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7982\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5122\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.8145\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6295\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7787\n",
      "5/5 [==============================] - 1s 77ms/step - loss: 0.4703\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for b in b_dataframe_list:\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'non_shiftable_load')\n",
    "        joblib.dump(xgb, 'my_models/models/Equipment_Electric_Power_model_b'+str(i)+'_new_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        lgb = MultiStepLightGBM(b,hyperparameter,'non_shiftable_load')\n",
    "        skforecast.utils.save_forecaster(lgb, 'my_models/models/LightGBM/Equipment_Electric_Power_model_b'+str(i)+'.h5', verbose=False)\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'non_shiftable_load')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/Equipment_Electric_Power_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/Equipment_Electric_Power_model_b'+str(i)+'.pkl')\n",
    "        \n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Bi_Model(b,hyperparameter,'non_shiftable_load')\n",
    "        lstm_model.save('my_models/models/LSTM_BiAttention/Equipment_Electric_Power_model_b'+str(i)+'_hyper.h5')\n",
    "\n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'non_shiftable_load')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/Equipment_Electric_Power_model_b'+str(i)+'_hyper.h5')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5143b8-6eb1-424a-975f-434b42934d7c",
   "metadata": {},
   "source": [
    "# Neighbourhood Level Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6668b0-ba09-47d6-84f6-a4b392d12377",
   "metadata": {},
   "source": [
    "### 1.) Carbon Intensity (kgCO2e/kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e4946-9e37-4a24-83a7-9ea2b756110e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine the datasets to one since we only have one CI \n",
    "comb = pd.concat([b_dataframe_list[0].reset_index(drop=True),\n",
    "                  b_dataframe_list[1].reset_index(drop=True),\n",
    "                  b_dataframe_list[2].reset_index(drop=True)])\n",
    "    \n",
    "if model_type == 'xgb':\n",
    "    xgb = XGBoost_Model(comb,hyperparameter,'carbon_intensity')\n",
    "    joblib.dump(xgb, 'my_models/models/Carbon_Intensity_Power_model_xgb.pkl')\n",
    "if model_type == 'lgb':\n",
    "    lgb = MultiStepLightGBM(comb,hyperparameter,'carbon_intensity')\n",
    "    skforecast.utils.save_forecaster(lgb, 'my_models/models/LightGBM/Carbon_Intensity_Power_model.h5', verbose=False)\n",
    "if model_type == 'fusion':\n",
    "    lgb_model  = LightGBM_Model(comb,hyperparameter,'carbon_intensity')\n",
    "    lstm_model = LSTM_Model(comb,hyperparameter,'carbon_intensity')\n",
    "        \n",
    "    joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/Carbon_Intensity_model.pkl')\n",
    "    joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/Carbon_Intensity_model.pkl')\n",
    "\n",
    "if model_type == 'lstm':\n",
    "    lstm_model = LSTM_Bi_Model(comb,hyperparameter,'carbon_intensity')\n",
    "    lstm_model.save('my_models/models/LSTM_BiAttention/Carbon_Intensity_model_hyper.h5')\n",
    "\n",
    "if model_type == 'bi-lstm':\n",
    "    lstm_model = LSTM_Bi_Model_hyper(comb,hyperparameter,'carbon_intensity')\n",
    "    lstm_model.save('my_models/models/LSTM_Bi_Model/Carbon_Intensity_model_hyper.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6048f-cd8c-4afb-ae16-da3e5e4bd5d8",
   "metadata": {},
   "source": [
    "### 2.) Solar Genetation (ConCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0146fac-ec4a-4bb8-a237-86bf867e2c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine the datasets to one since we only have one CI \n",
    "comb = pd.concat([b_dataframe_list[0].reset_index(drop=True),\n",
    "                  b_dataframe_list[1].reset_index(drop=True),\n",
    "                  b_dataframe_list[2].reset_index(drop=True)])\n",
    "    \n",
    "if model_type == 'xgb':\n",
    "    xgb = XGBoost_Model(comb,hyperparameter,'solar_generation')\n",
    "    joblib.dump(xgb, 'my_models/models/solar_generation_Power_model_xgb.pkl')\n",
    "if model_type == 'lgb':\n",
    "    lgb = MultiStepLightGBM(comb,hyperparameter,'solar_generation')\n",
    "    skforecast.utils.save_forecaster(lgb, 'my_models/models/LightGBM/solar_generation_model.h5', verbose=False)\n",
    "if model_type == 'fusion':\n",
    "    lgb_model  = LightGBM_Model(comb,hyperparameter,'solar_generation')\n",
    "    lstm_model = LSTM_Model(comb,hyperparameter,'carbon_intensity')\n",
    "        \n",
    "    joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/solar_generation_model.pkl')\n",
    "    joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/solar_generation_model_scaled.pkl')\n",
    "\n",
    "if model_type == 'lstm':\n",
    "    lstm_model = LSTM_Bi_Model(comb,hyperparameter,'solar_generation')\n",
    "    lstm_model.save('my_models/models/LSTM_BiAttention/solar_generation_model_hyper.h5')\n",
    "\n",
    "if model_type == 'bi-lstm':\n",
    "    lstm_model = LSTM_Bi_Model_hyper(comb,hyperparameter,'solar_generation')\n",
    "    lstm_model.save('my_models/models/LSTM_Bi_Model/solar_generation_model_hyper.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99ab8e-9d57-4554-96b5-6f69cc0041f3",
   "metadata": {},
   "source": [
    "### 2.) Solar Generation (W/kW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d128e16-9ebc-4318-864e-53f440826011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "for b in b_dataframe_list:\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'solar_generation')\n",
    "        joblib.dump(xgb, 'my_models/models/solar_generation_model_b'+str(i)+'_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        lgb = LightGBM_Model(b,hyperparameter,'solar_generation')\n",
    "        joblib.dump(lgb, 'my_models/models/solar_generation_model_b'+str(i)+'_lightgbm.pkl')\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'solar_generation')\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'solar_generation')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/solar_generation_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/solar_generation_model_b'+str(i)+'.pkl')\n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'solar_generation')\n",
    "        lstm_model.save('my_models/models/LSTM/solar_generation_model_b'+str(i)+'.h5')\n",
    "\n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'solar_generation')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/solar_generation_model_b'+str(i)+'_hyper.h5')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ab787-e32b-46d3-85ae-62dac941cd32",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e126c5a-4598-475a-a22b-001c1d1da927",
   "metadata": {},
   "source": [
    "### LightGBM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dbe62f7-aa7c-4b50-88d1-7911ef439a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LightGBM_Model(b,hpt,feature):\n",
    "\n",
    "    # Load the feature selection\n",
    "    f_l = pd.read_csv('data/features/feature_importance_cooling_demand.csv')\n",
    "    \n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b#[f_l['feature']]\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    if hpt == True:\n",
    "        params = {\n",
    "            'max_depth':        [3, 4, 5],\n",
    "            'num_leaves':       [10, 15, 20],\n",
    "            'learning_rate':    [0.05, 0.1, 0.15],\n",
    "            'n_estimators':     [50, 100, 200],\n",
    "            'subsample':        [0.5, 0.7, 0.9],\n",
    "            'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "            'reg_alpha':        [0.01, 0.1, 1],\n",
    "            'reg_lambda':       [0.01, 0.1, 1],\n",
    "            'steps':            [48],\n",
    "            'verbose':[-1]\n",
    "        }\n",
    "    \n",
    "        lgb_mean = LGBMRegressor(boosting_type='gbdt', objective='regression')\n",
    "        grid_search_mean = GridSearchCV(lgb_mean, params, cv=5, n_jobs=-1)\n",
    "        grid_search_mean.fit(X_train, y_train)\n",
    "        \n",
    "        # Create an AdaBoost model with LightGBM as the base estimator\n",
    "        #adaboost_model = AdaBoostRegressor(base_estimator=grid_search_mean, n_estimators=50)\n",
    "        #adaboost_model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return grid_search_mean\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        lgb_params = {\n",
    "        'n_jobs': 1,\n",
    "        'max_depth': 4,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'subsample': 0.9,\n",
    "        'n_estimators': 80,\n",
    "        'learning_rate': 0.1,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'steps':48,\n",
    "        'verbose':-1,\n",
    "        }\n",
    "        \n",
    "        # fitting the model\n",
    "        gbm = LGBMRegressor(**lgb_params)\n",
    "        gbm.fit(X_train, y_train)\n",
    "        \n",
    "        # Create an AdaBoost model with LightGBM as the base estimator\n",
    "        #adaboost_model = AdaBoostRegressor(base_estimator=gbm, n_estimators=50)\n",
    "        #adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "        return gbm#adaboost_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a9ce8-9b05-444b-ad08-3e8b348dae11",
   "metadata": {},
   "source": [
    "### LightGBM Multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "279b92a0-7cef-4034-a775-3f0bb416684c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n",
    "\n",
    "\n",
    "def MultiStepLightGBM(b,hpt,feature):\n",
    "    \n",
    "    lgb_params = {\n",
    "        'n_jobs': 1,\n",
    "        'max_depth': 4,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'subsample': 0.9,\n",
    "        'n_estimators': 80,\n",
    "        'learning_rate': 0.1,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'verbose':-1,\n",
    "        }\n",
    "    \n",
    "    features = b\n",
    "    target   = b[feature]\n",
    "    \n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    forecaster = ForecasterAutoregMultiVariate(\n",
    "                     regressor          = LGBMRegressor(**lgb_params),\n",
    "                     level              = feature,\n",
    "                     lags               = 24,\n",
    "                     steps              = 48,\n",
    "                     transformer_series = None,\n",
    "                     transformer_exog   = None,\n",
    "                     weight_func        = None,\n",
    "                     n_jobs             = 'auto'\n",
    "             )\n",
    "\n",
    "    forecaster.fit(series=X_train)\n",
    "    \n",
    "    return forecaster\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a4eb7-7354-489d-9be9-5b96f581b0f7",
   "metadata": {},
   "source": [
    "### CatBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1069a8-60aa-44ec-9c7f-dbd6588ecb5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CatBoost_Model(b,hpt,feature):\n",
    "    \n",
    "    # Load the feature selection\n",
    "    f_l = pd.read_csv('data/features/feature_importance_'+str(feature)+'.csv')\n",
    "    \n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b#[f_l['feature']]\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    reg = XGBRegressor(n_estimators=100)\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            early_stopping_rounds=50)\n",
    "\n",
    "    return reg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0578c-a0d3-4710-85aa-10d0fe5b25f4",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d109d235-255f-43cf-84ef-bafddbc5d8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSTM_Bi_Model_hyper(b, hyperparameter, feature):\n",
    "    \n",
    "    # Generate the x,y\n",
    "    X = b\n",
    "    y = b[feature]\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "    # Reshape data for LSTM input (samples, sequence_length, num_features)\n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    def objective(params, X_train, X_val, y_train, y_val):\n",
    "        units, learning_rate, dropout_input, dropout_lstm, num_layers, batch_size = params\n",
    "\n",
    "        # Reshape the input data to match the LSTM input shape\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_input, recurrent_dropout=dropout_lstm), input_shape=input_shape))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            model.add(Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_lstm, recurrent_dropout=dropout_lstm)))\n",
    "\n",
    "        model.add(LSTM(units=units, dropout=dropout_lstm, recurrent_dropout=dropout_lstm))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "        val_loss = model.evaluate(X_val, y_val)\n",
    "        return val_loss\n",
    "\n",
    "    space = [\n",
    "        (50, 150),                           # Number of LSTM units (units in each LSTM layer)\n",
    "        (1e-6, 1e-2, 'log-uniform'),         # Learning rate\n",
    "        (0.1, 0.9),                          # Dropout rate for input layer\n",
    "        (0.1, 0.9),                          # Dropout rate for LSTM layers\n",
    "        (1, 3),                              # Number of LSTM layers\n",
    "        (10, 100)                            # Batch size\n",
    "    ]\n",
    "\n",
    "    # Create a partial function without fixed arguments\n",
    "    objective_partial = partial(objective, X_train=X_train, X_val=X_test, y_train=y_train, y_val=y_test)\n",
    "\n",
    "    # Perform hyperparameter optimization\n",
    "    result = gp_minimize(objective_partial, space, n_calls=20, random_state=42)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_units, best_learning_rate, best_dropout_input, best_dropout_lstm, best_num_layers, best_batch_size = result.x\n",
    "    best_val_loss = result.fun\n",
    "\n",
    "    # Train the final model using the best hyperparameters\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_input, recurrent_dropout=best_dropout_lstm), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "    for _ in range(best_num_layers - 1):\n",
    "        best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm)))\n",
    "    \n",
    "    best_model.add(LSTM(units=best_units, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm))\n",
    "    best_model.add(Dense(1))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=best_learning_rate)\n",
    "    best_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    best_model.fit(X_train, y_train, epochs=1000, batch_size=best_batch_size, validation_data=(X_test, y_test), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef75ce8-c244-4aed-a242-ce004b5d765a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "def LSTM_Bi_Model(b,hpt,feature):\n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    \n",
    "    # Reshape data for LSTM input (samples, sequence_length, num_features)\n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "\n",
    "    def objective_attention(params, X_train, X_val, y_train, y_val):\n",
    "        units, learning_rate, dropout_input, dropout_lstm, num_layers, batch_size = params\n",
    "\n",
    "        # Reshape the input data to match the LSTM input shape\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        # Input layer\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        lstm_output = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_input, recurrent_dropout=dropout_lstm))(inputs)\n",
    "        for _ in range(num_layers - 1):\n",
    "            lstm_output = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_lstm, recurrent_dropout=dropout_lstm))(lstm_output)\n",
    "\n",
    "        # BahdanauAttention layer\n",
    "        context_vector, attention_weights = BahdanauAttention(units)(lstm_output, lstm_output)\n",
    "\n",
    "        # Concatenate context vector with LSTM output\n",
    "        lstm_output_combined = Concatenate()([context_vector, lstm_output])\n",
    "\n",
    "        # LSTM layer with return_sequences=False, go_backwards=True\n",
    "        lstm = LSTM(units=units, return_sequences=False, go_backwards=True)(lstm_output_combined)\n",
    "\n",
    "        # Output layer\n",
    "        output = Dense(48)(lstm)\n",
    "\n",
    "        # Create the model\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "        val_loss = model.evaluate(X_val, y_val)\n",
    "        return val_loss    \n",
    "    \n",
    "    \n",
    "    \n",
    "    space = [\n",
    "        (50, 150),                           # Number of LSTM units (units in each LSTM layer)\n",
    "        (1e-6, 1e-2, 'log-uniform'),         # Learning rate\n",
    "        (0.1, 0.9),                          # Dropout rate for input layer\n",
    "        (0.1, 0.9),                          # Dropout rate for LSTM layers\n",
    "        (1, 3),                              # Number of LSTM layers\n",
    "        (10, 100)                            # Batch size\n",
    "    ]\n",
    "\n",
    "    # Create a partial function without fixed arguments\n",
    "    objective_partial = partial(objective_attention, X_train=X_train, X_val=X_test, y_train=y_train, y_val=y_test)\n",
    "\n",
    "    # Perform hyperparameter optimization\n",
    "    result = gp_minimize(objective_partial, space, n_calls=20, random_state=42)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_units, best_learning_rate, best_dropout_input, best_dropout_lstm, best_num_layers, best_batch_size = result.x\n",
    "    best_val_loss = result.fun\n",
    "\n",
    "    # Train the final model using the best hyperparameters\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_input, recurrent_dropout=best_dropout_lstm), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "    for _ in range(best_num_layers - 1):\n",
    "        best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm)))\n",
    "    \n",
    "    best_model.add(LSTM(units=best_units, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm))\n",
    "    best_model.add(Dense(48))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=best_learning_rate)\n",
    "    best_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    best_model.fit(X_train, y_train, epochs=1000, batch_size=best_batch_size, validation_data=(X_test, y_test), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    return best_model\n",
    "    \n",
    "    # BiDirectional LSTM with Attention:\n",
    "    #inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    #bidirectional_lstm = Bidirectional(LSTM(units=50, return_sequences=True))(inputs)\n",
    "    #bidirectional_lstm2 = Bidirectional(LSTM(units=50, return_sequences=True))(bidirectional_lstm)\n",
    "    #lstm, forward_h, forward_c, backward_h, backward_c = LSTM(units=50, return_state=True, go_backwards=True)(bidirectional_lstm2)\n",
    "    #state_h = Concatenate()([forward_h, backward_h])\n",
    "    #context_vector, attention_weights = BahdanauAttention(50)(lstm, lstm)\n",
    "\n",
    "    #output = Dense(units=48)(context_vector)\n",
    "    #model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Use early stopping to prevent overfitting\n",
    "    #early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    #model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping],verbose=0)\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd234c78-5aab-462a-8c7c-11c51c70f894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96430fdc-557d-4e7f-9995-3c6ad418da3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from keras.layers import Bidirectional\n",
    "\n",
    "def LSTM_Model(b,hpt,feature):\n",
    "\n",
    "    # Generate the x,y\n",
    "    features = b\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Reshape data for LSTM input (samples, sequence_length, num_features)\n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # LSTM with Attention: \n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    lstm = LSTM(units=50, return_sequences=True)(inputs)\n",
    "    lstm, forward_h, forward_c = LSTM(units=50, return_state=True, go_backwards=True)(lstm)\n",
    "    state_h = Concatenate()([forward_h, forward_c])\n",
    "    context_vector, attention_weights = BahdanauAttention(50)(lstm, lstm)\n",
    "\n",
    "    output = Dense(units=48)(context_vector)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "\n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping],verbose=0)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0169dbb8-7839-4245-984c-9f59b426562a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate\n",
    "import tensorflow as tf\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the custom layer here if necessary\n",
    "        super(BahdanauAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # Expand dimensions for broadcasting\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # Calculate context vector\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "        }\n",
    "        base_config = super(BahdanauAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e4371-f61f-4ea4-9a33-ed3ae5c55ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d2c58-43d7-4275-9b7f-759c4012cb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "024ea8fa-812f-4f1e-b0c0-ae8886e8c379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Fusion_Model(b,hpt,feature,lstm_model,lightgbm_model):\n",
    "    \n",
    "    # Load the feature selection\n",
    "    f_l = pd.read_csv('data/features/feature_importance_'+str(feature)+'.csv')\n",
    "    \n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b#[f_l['feature']]\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # Create an ensemble model using VotingRegressor\n",
    "    ensemble_model = VotingRegressor(estimators=[('lstm', lstm_model), ('lgbm', lightgbm_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    return ensemble_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b38e0e-c673-41fe-8a97-b1890cc25c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6c123-ac24-4957-b317-74a0d5ec6731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ce45e-3ec4-4765-a8da-5568a4e5aabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f4244-379d-4021-bd9a-dd69ce80ecc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ba147-6d10-4b20-8cb5-56cb16f6cc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560235de-5577-4919-bdf6-1021ef6f79e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b85efd-8c1a-43bb-82a3-9d1ca6e195fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc2b52-6236-4670-89da-86023cbed3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cb36f-9613-48eb-90be-cffe73c682b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0fd6a-fc08-4de4-9682-9cc373d5a018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fa69f-e93f-427d-bee1-d415a68da321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725c757-8501-4b1e-b9cc-b9e71372a9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0a531-5522-4c3a-a99f-fb61b9d50d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cde1c7-050d-4667-9ba7-2e0a777a7771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
