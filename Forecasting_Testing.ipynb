{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a114b7b5-58cd-4a68-b82c-766a76667c46",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24d048c-bed0-4f82-a9b3-e4ea7d51e806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.1.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.10.1)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.1.0\n",
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (1.10.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (0.13.5)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (2.0.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category_encoders) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders) (23.1)\n",
      "Installing collected packages: category_encoders\n",
      "Successfully installed category_encoders-2.6.3\n",
      "Collecting citylearn==2.1b9\n",
      "  Downloading CityLearn-2.1b9-py3-none-any.whl (16.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gym>=0.21.0 (from citylearn==2.1b9)\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from citylearn==2.1b9) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from citylearn==2.1b9) (1.23.5)\n",
      "Collecting pandas==1.3.5 (from citylearn==2.1b9)\n",
      "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.0.2 (from citylearn==2.1b9)\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting simplejson (from citylearn==2.1b9)\n",
      "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from citylearn==2.1b9) (1.12.0+cu116)\n",
      "Requirement already satisfied: torchvision>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from citylearn==2.1b9) (0.13.0+cu116)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas==1.3.5->citylearn==2.1b9) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas==1.3.5->citylearn==2.1b9) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2->citylearn==2.1b9) (1.10.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2->citylearn==2.1b9) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2->citylearn==2.1b9) (3.1.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym>=0.21.0->citylearn==2.1b9) (2.2.1)\n",
      "Collecting gym-notices>=0.0.4 (from gym>=0.21.0->citylearn==2.1b9)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->citylearn==2.1b9) (4.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.13.0->citylearn==2.1b9) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.13.0->citylearn==2.1b9) (9.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->citylearn==2.1b9) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->citylearn==2.1b9) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->citylearn==2.1b9) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->citylearn==2.1b9) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->citylearn==2.1b9) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->citylearn==2.1b9) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5->citylearn==2.1b9) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.13.0->citylearn==2.1b9) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.13.0->citylearn==2.1b9) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.13.0->citylearn==2.1b9) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.13.0->citylearn==2.1b9) (2023.7.22)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827620 sha256=21dbb20c45dffb00d0e4e50c0880e5a4e6e8210e909167a2d7f189357396178b\n",
      "  Stored in directory: /home/philaupk/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, simplejson, gym, scikit-learn, pandas, citylearn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.0\n",
      "    Uninstalling pandas-2.0.0:\n",
      "      Successfully uninstalled pandas-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mesa 2.1.1 requires solara, which is not installed.\n",
      "stable-baselines3 2.1.0 requires torch>=1.13, but you have torch 1.12.0+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed citylearn-2.1b9 gym-0.26.2 gym-notices-0.0.8 pandas-1.3.5 scikit-learn-1.0.2 simplejson-3.19.2\n",
      "Collecting keras==2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.13.1\n",
      "    Uninstalling keras-2.13.1:\n",
      "      Successfully uninstalled keras-2.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.13.1 requires keras<2.14,>=2.13.1, but you have keras 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.12.0\n",
      "Collecting tensorflow==2.12.0\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.59.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (3.8.0)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.20-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (16.0.6)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (4.21.12)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (67.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (1.16.0)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (4.5.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.12.0) (0.34.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.40.0)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
      "Installing collected packages: wrapt, tensorflow-estimator, ml-dtypes, jax, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.15.0\n",
      "    Uninstalling wrapt-1.15.0:\n",
      "      Successfully uninstalled wrapt-1.15.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.13.0\n",
      "    Uninstalling tensorflow-estimator-2.13.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.13.0\n",
      "    Uninstalling tensorboard-2.13.0:\n",
      "      Successfully uninstalled tensorboard-2.13.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.13.1\n",
      "    Uninstalling tensorflow-2.13.1:\n",
      "      Successfully uninstalled tensorflow-2.13.1\n",
      "Successfully installed jax-0.4.20 ml-dtypes-0.3.1 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n",
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.2.0)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize)\n",
      "  Downloading pyaml-23.9.7-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize) (1.0.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-23.9.7 scikit-optimize-0.9.0\n",
      "Collecting sktime\n",
      "  Downloading sktime-0.24.1-py3-none-any.whl (20.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/20.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.27,>=1.21 in /opt/conda/lib/python3.10/site-packages (from sktime) (1.23.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from sktime) (23.1)\n",
      "Requirement already satisfied: pandas<2.2.0,>=1.1 in /opt/conda/lib/python3.10/site-packages (from sktime) (1.3.5)\n",
      "Collecting scikit-base<0.7.0 (from sktime)\n",
      "  Downloading scikit_base-0.6.1-py3-none-any.whl (122 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<1.4.0,>=0.24 in /opt/conda/lib/python3.10/site-packages (from sktime) (1.0.2)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2 in /opt/conda/lib/python3.10/site-packages (from sktime) (1.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.1->sktime) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.1->sktime) (2023.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.0,>=0.24->sktime) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.0,>=0.24->sktime) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas<2.2.0,>=1.1->sktime) (1.16.0)\n",
      "Installing collected packages: scikit-base, sktime\n",
      "Successfully installed scikit-base-0.6.1 sktime-0.24.1\n",
      "Collecting skforecast\n",
      "  Downloading skforecast-0.10.1-py3-none-any.whl (397 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.3/397.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.26,>=1.20 in /opt/conda/lib/python3.10/site-packages (from skforecast) (1.23.5)\n",
      "Requirement already satisfied: pandas<2.1,>=1.2 in /opt/conda/lib/python3.10/site-packages (from skforecast) (1.3.5)\n",
      "Requirement already satisfied: tqdm<4.66,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from skforecast) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn<1.4,>=1.0 in /opt/conda/lib/python3.10/site-packages (from skforecast) (1.0.2)\n",
      "Collecting optuna<3.3,>=2.10.0 (from skforecast)\n",
      "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from skforecast) (1.2.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (1.10.3)\n",
      "Collecting cmaes>=0.9.1 (from optuna<3.3,>=2.10.0->skforecast)\n",
      "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
      "Collecting colorlog (from optuna<3.3,>=2.10.0->skforecast)\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (2.0.9)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna<3.3,>=2.10.0->skforecast) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas<2.1,>=1.2->skforecast) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4,>=1.0->skforecast) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4,>=1.0->skforecast) (3.1.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas<2.1,>=1.2->skforecast) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna<3.3,>=2.10.0->skforecast) (2.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna<3.3,>=2.10.0->skforecast) (2.1.2)\n",
      "Installing collected packages: colorlog, cmaes, optuna, skforecast\n",
      "Successfully installed cmaes-0.10.0 colorlog-6.7.0 optuna-3.2.0 skforecast-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install category_encoders\n",
    "!pip install citylearn==2.1b9\n",
    "\n",
    "\n",
    "# Because of the Wrapper Error\n",
    "!pip install keras==2.12.0\n",
    "!pip install tensorflow==2.12.0\n",
    "\n",
    "# Then you have to reinitiate the kernel! and install scikit-optimize!\n",
    "!pip install scikit-optimize\n",
    "!pip install sktime\n",
    "!pip install skforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4071c164-b5b6-4eae-910f-081322985fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from my_models.user_model import SubmissionModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import gp_minimize\n",
    "from functools import partial\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import skforecast as skforecast\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5489a18-4f56-45ac-b34d-2cf9bf6bb33f",
   "metadata": {},
   "source": [
    "## Simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05333283-0a9d-4ca3-ae1c-6ce1378e18a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a test env\n",
    "class WrapperEnv:\n",
    "    \"\"\"\n",
    "    Env to wrap provide Citylearn Env data without providing full env\n",
    "    Preventing attribute access outside of the available functions\n",
    "    \"\"\"\n",
    "    def __init__(self, env_data):\n",
    "        self.observation_names = env_data['observation_names']\n",
    "        self.action_names = env_data['action_names']\n",
    "        self.observation_space = env_data['observation_space']\n",
    "        self.action_space = env_data['action_space']\n",
    "        self.time_steps = env_data['time_steps']\n",
    "        self.seconds_per_time_step = env_data['seconds_per_time_step']\n",
    "        self.random_seed = env_data['random_seed']\n",
    "        self.buildings_metadata = env_data['buildings_metadata']\n",
    "        self.episode_tracker = env_data['episode_tracker']\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        return {'buildings': self.buildings_metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0049ca9f-53b0-417c-9f3b-13293825d09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_citylearn_env(config):\n",
    "    env = CityLearnEnv(config.SCHEMA)\n",
    "\n",
    "    env_data = dict(\n",
    "        observation_names = env.observation_names,\n",
    "        action_names = env.action_names,\n",
    "        observation_space = env.observation_space,\n",
    "        action_space = env.action_space,\n",
    "        time_steps = 35039,\n",
    "        buildings_metadata = env.get_metadata()['buildings'],\n",
    "        num_buildings = len(env.buildings),\n",
    "        building_names = [b.name for b in env.buildings],\n",
    "        b0_pv_capacity = env.buildings[0].pv.nominal_power,\n",
    "    )\n",
    "\n",
    "    # Turn off actions for all buildings and do not simulate power outage (forecasting only).\n",
    "    for b in env.buildings:\n",
    "        b.ignore_dynamics = True\n",
    "        b.simulate_power_outage = False\n",
    "\n",
    "    return env, env_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fe70e10-5cce-4035-8bd9-7486201d83b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    data_dir = './data/'\n",
    "    SCHEMA = os.path.join(data_dir, 'schemas/warm_up/schema.json')\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adf5221e-57fb-44ee-85b0-1fa6605e63b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env, env_data = create_citylearn_env(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15054c39-c5e2-477f-93fa-0a22f69b6242",
   "metadata": {},
   "source": [
    "## Generation of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1783afd-efc0-4205-aa31-3bda2673da08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "\n",
    "b_1_dataframe = pd.DataFrame(columns=['day_type', 'hour', 'outdoor_dry_bulb_temperature', 'outdoor_dry_bulb_temperature_predicted_6h', \n",
    "                                      'outdoor_dry_bulb_temperature_predicted_12h', 'outdoor_dry_bulb_temperature_predicted_24h', 'diffuse_solar_irradiance',\n",
    "                                      'diffuse_solar_irradiance_predicted_6h', 'diffuse_solar_irradiance_predicted_12h', \n",
    "                                      'diffuse_solar_irradiance_predicted_24h', 'direct_solar_irradiance', 'direct_solar_irradiance_predicted_6h',\n",
    "                                      'direct_solar_irradiance_predicted_12h', 'direct_solar_irradiance_predicted_24h', 'carbon_intensity', \n",
    "                                      'indoor_dry_bulb_temperature', 'non_shiftable_load', 'solar_generation', 'dhw_storage_soc', 'electrical_storage_soc', \n",
    "                                      'electricity_pricing', 'electricity_pricing_predicted_6h', \n",
    "                                      'electricity_pricing_predicted_12h', 'electricity_pricing_predicted_24h', 'cooling_demand',\n",
    "                                      'dhw_demand','indoor_dry_bulb_temperature_set_point','occupant_count','net_electricity_consumption'])\n",
    "\n",
    "b_2_dataframe = pd.DataFrame(columns=['day_type', 'hour', 'outdoor_dry_bulb_temperature', 'outdoor_dry_bulb_temperature_predicted_6h', \n",
    "                                      'outdoor_dry_bulb_temperature_predicted_12h', 'outdoor_dry_bulb_temperature_predicted_24h', 'diffuse_solar_irradiance',\n",
    "                                      'diffuse_solar_irradiance_predicted_6h', 'diffuse_solar_irradiance_predicted_12h', \n",
    "                                      'diffuse_solar_irradiance_predicted_24h', 'direct_solar_irradiance', 'direct_solar_irradiance_predicted_6h',\n",
    "                                      'direct_solar_irradiance_predicted_12h', 'direct_solar_irradiance_predicted_24h', 'carbon_intensity', \n",
    "                                      'indoor_dry_bulb_temperature', 'non_shiftable_load', 'solar_generation', 'dhw_storage_soc', 'electrical_storage_soc', \n",
    "                                      'electricity_pricing', 'electricity_pricing_predicted_6h', \n",
    "                                      'electricity_pricing_predicted_12h', 'electricity_pricing_predicted_24h', 'cooling_demand',\n",
    "                                      'dhw_demand','indoor_dry_bulb_temperature_set_point','occupant_count','net_electricity_consumption'])\n",
    "\n",
    "b_3_dataframe = pd.DataFrame(columns=['day_type', 'hour', 'outdoor_dry_bulb_temperature', 'outdoor_dry_bulb_temperature_predicted_6h', \n",
    "                                      'outdoor_dry_bulb_temperature_predicted_12h', 'outdoor_dry_bulb_temperature_predicted_24h', 'diffuse_solar_irradiance',\n",
    "                                      'diffuse_solar_irradiance_predicted_6h', 'diffuse_solar_irradiance_predicted_12h', \n",
    "                                      'diffuse_solar_irradiance_predicted_24h', 'direct_solar_irradiance', 'direct_solar_irradiance_predicted_6h',\n",
    "                                      'direct_solar_irradiance_predicted_12h', 'direct_solar_irradiance_predicted_24h', 'carbon_intensity', \n",
    "                                      'indoor_dry_bulb_temperature', 'non_shiftable_load', 'solar_generation', 'dhw_storage_soc', 'electrical_storage_soc', \n",
    "                                      'electricity_pricing', 'electricity_pricing_predicted_6h', \n",
    "                                      'electricity_pricing_predicted_12h', 'electricity_pricing_predicted_24h', 'cooling_demand',\n",
    "                                      'dhw_demand','indoor_dry_bulb_temperature_set_point','occupant_count','net_electricity_consumption'])\n",
    "\n",
    "b_dataframe_list = [b_1_dataframe,b_2_dataframe,b_3_dataframe]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70c42120-ce5a-46d1-af63-a279e2425823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.67788136], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, b in enumerate(env.buildings):\n",
    "    \n",
    "    con = b.net_electricity_consumption\n",
    "    print(str(len(con)))\n",
    "    \n",
    "env.buildings[0].net_electricity_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0db48ddf-2417-4ca8-9663-70c05a33e7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the Datasets for the different buildings:\n",
    "# Here I only need to simulate the ones, which are not present in the dataset:\n",
    "\n",
    "for idx, b in enumerate(env.buildings):\n",
    "    indoor_dry_bulb_temperature           = b.energy_simulation.indoor_dry_bulb_temperature\n",
    "    non_shiftable_load                    = b.energy_simulation.non_shiftable_load\n",
    "    solar_generation                      = b.energy_simulation.solar_generation\n",
    "    dhw_storage_soc                       = b.dhw_storage.soc\n",
    "    electrical_storage_soc                = b.electrical_storage.soc\n",
    "    cooling_demand                        = b.energy_simulation.cooling_demand\n",
    "    dhw_demand                            = b.energy_simulation.dhw_demand\n",
    "    indoor_dry_bulb_temperature_set_point = b.energy_simulation.indoor_dry_bulb_temperature_set_point\n",
    "    occupant_count                        = b.occupant_count.repeat(720)\n",
    "    net_electricity_consumption           = b.net_electricity_consumption.repeat(720)\n",
    "    \n",
    "    # After the generation of the different features I will add the global features (which are independend from the houses!)\n",
    "    day_type         = env.buildings[0].energy_simulation.day_type\n",
    "    hour             = env.buildings[0].energy_simulation.hour\n",
    "    carbon_intensity = env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "    # Loading the local features\n",
    "    filepath = 'data/schemas/warm_up/'\n",
    "\n",
    "    pricing    = pd.read_csv(filepath + 'pricing.csv')\n",
    "    weather    = pd.read_csv(filepath + 'weather.csv')\n",
    "\n",
    "    electricity_pricing                = pricing['Electricity Pricing [$/kWh]']\n",
    "    electricity_pricing_predicted_6h   = pricing['6h Prediction Electricity Pricing [$/kWh]']\n",
    "    electricity_pricing_predicted_12h  = pricing['12h Prediction Electricity Pricing [$/kWh]']\n",
    "    electricity_pricing_predicted_24h  = pricing['24h Prediction Electricity Pricing [$/kWh]']\n",
    "\n",
    "    outdoor_dry_bulb_temperature                = weather['Outdoor Drybulb Temperature (C)']\n",
    "    outdoor_dry_bulb_temperature_predicted_6h   = weather['6h Outdoor Drybulb Temperature (C)']\n",
    "    outdoor_dry_bulb_temperature_predicted_12h  = weather['12h Outdoor Drybulb Temperature (C)']\n",
    "    outdoor_dry_bulb_temperature_predicted_24h  = weather['24h Outdoor Drybulb Temperature (C)']\n",
    "\n",
    "    diffuse_solar_irradiance                    = weather['Diffuse Solar Radiation (W/m2)']\n",
    "    diffuse_solar_irradiance_predicted_6h       = weather['6h Diffuse Solar Radiation (W/m2)']\n",
    "    diffuse_solar_irradiance_predicted_12h      = weather['12h Diffuse Solar Radiation (W/m2)']\n",
    "    diffuse_solar_irradiance_predicted_24h      = weather['24h Diffuse Solar Radiation (W/m2)']\n",
    "\n",
    "    direct_solar_irradiance                     = weather['Direct Solar Radiation (W/m2)']\n",
    "    direct_solar_irradiance_predicted_6h        = weather['6h Direct Solar Radiation (W/m2)']\n",
    "    direct_solar_irradiance_predicted_12h       = weather['12h Direct Solar Radiation (W/m2)']\n",
    "    direct_solar_irradiance_predicted_24h       = weather['24h Direct Solar Radiation (W/m2)']\n",
    "    \n",
    "    # Generate the Dataframe for the training\n",
    "    b_dataframe_list[idx]['day_type']                                   = day_type\n",
    "    b_dataframe_list[idx]['hour']                                       = hour\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature']               = outdoor_dry_bulb_temperature\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature_predicted_6h']  = outdoor_dry_bulb_temperature_predicted_6h\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature_predicted_12h'] = outdoor_dry_bulb_temperature_predicted_12h\n",
    "    b_dataframe_list[idx]['outdoor_dry_bulb_temperature_predicted_24h'] = outdoor_dry_bulb_temperature_predicted_24h\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance']                   = diffuse_solar_irradiance\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance_predicted_6h']      = diffuse_solar_irradiance_predicted_6h\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance_predicted_12h']     = diffuse_solar_irradiance_predicted_12h\n",
    "    b_dataframe_list[idx]['diffuse_solar_irradiance_predicted_24h']     = diffuse_solar_irradiance_predicted_24h\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance']                    = direct_solar_irradiance\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance_predicted_6h']       = direct_solar_irradiance_predicted_6h\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance_predicted_12h']      = direct_solar_irradiance_predicted_12h\n",
    "    b_dataframe_list[idx]['direct_solar_irradiance_predicted_24h']      = direct_solar_irradiance_predicted_24h\n",
    "    b_dataframe_list[idx]['carbon_intensity']                           = carbon_intensity\n",
    "    b_dataframe_list[idx]['indoor_dry_bulb_temperature']                = indoor_dry_bulb_temperature\n",
    "    b_dataframe_list[idx]['non_shiftable_load']                         = non_shiftable_load\n",
    "    b_dataframe_list[idx]['solar_generation']                           = solar_generation\n",
    "    b_dataframe_list[idx]['dhw_storage_soc']                            = dhw_storage_soc\n",
    "    b_dataframe_list[idx]['electrical_storage_soc']                     = electrical_storage_soc\n",
    "    b_dataframe_list[idx]['electricity_pricing']                        = electricity_pricing\n",
    "    b_dataframe_list[idx]['electricity_pricing_predicted_6h']           = electricity_pricing_predicted_6h\n",
    "    b_dataframe_list[idx]['electricity_pricing_predicted_12h']          = electricity_pricing_predicted_12h\n",
    "    b_dataframe_list[idx]['electricity_pricing_predicted_24h']          = electricity_pricing_predicted_24h\n",
    "    b_dataframe_list[idx]['cooling_demand']                             = cooling_demand\n",
    "    b_dataframe_list[idx]['dhw_demand']                                 = dhw_demand\n",
    "    b_dataframe_list[idx]['indoor_dry_bulb_temperature_set_point']      = indoor_dry_bulb_temperature_set_point\n",
    "    b_dataframe_list[idx]['occupant_count']                             = occupant_count\n",
    "    b_dataframe_list[idx]['net_electricity_consumption']                = net_electricity_consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36edbc-8963-4cf9-853e-c087c493193c",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402aa4b-066a-4024-806a-f07878d5f764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the important features into files\n",
    "b = 1\n",
    "for data in b_dataframe_list:\n",
    "    feature_selection(data,'cooling_demand')\n",
    "    feature_selection(data,'dhw_demand')\n",
    "    feature_selection(data,'non_shiftable_load')\n",
    "    feature_selection(data,'carbon_intensity')\n",
    "    feature_selection(data,'solar_generation')\n",
    "    b = b + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc97c1a-68e1-4d74-a6e1-301ff68e91a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_selection(data,obs_feature):\n",
    "    # Split the dataset into features and target\n",
    "    X = data\n",
    "    y = data[obs_feature]\n",
    "    \n",
    "    # Apply Information Gain\n",
    "    ig = mutual_info_regression(X, y)\n",
    "\n",
    "    # Create a dictionary of feature importance scores\n",
    "    feature_scores = {}\n",
    "    i = 0\n",
    "    for (columnName, columnData) in data.items():\n",
    "        feature_scores[columnName] = ig[i]\n",
    "        i = i + 1\n",
    "    # Sort the features by importance score in descending order\n",
    "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    f_l = []\n",
    "    s_l = []\n",
    "    a_l = []\n",
    "    a_l_s = []\n",
    "    # Print the feature importance scores and the sorted features\n",
    "    for feature, score in sorted_features:\n",
    "        a_l.append(feature)\n",
    "        a_l_s.append(score)\n",
    "        if score > 0.10:\n",
    "            # save the features\n",
    "            f_l.append(feature)\n",
    "            s_l.append(score)\n",
    "            \n",
    "    dic = {'feature': f_l, 'score': s_l}\n",
    "    dic_a = {'feature': a_l, 'score': a_l_s}\n",
    "    df2 = pd.DataFrame(dic_a)\n",
    "    df = pd.DataFrame(dic)\n",
    "    df.to_csv('data/features/feature_importance_'+str(obs_feature)+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84e00c-1958-4f58-9c60-8de58a0e39e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Predictors (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fd673-11e2-4f20-a660-fb36934ccd2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'lgb'\n",
    "hyperparameter = False\n",
    "\n",
    "if 'BahdanauAttention' not in tf.keras.utils.get_custom_objects():\n",
    "    register_keras_serializable('BahdanauAttention')(BahdanauAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0382c-bd19-474a-b753-fa3e54dbeb9f",
   "metadata": {},
   "source": [
    "## Building Level Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a8265-0e7c-4c65-836c-6bb7a55701e1",
   "metadata": {},
   "source": [
    "### 1.) Cooling Load (kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45da7752-a409-4bf3-a5ed-fc0d82380079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------_-------------\n",
      "29\n",
      "---------_-------------\n",
      "29\n",
      "---------_-------------\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 1\n",
    "for b in b_dataframe_list:\n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'cooling_demand')\n",
    "        joblib.dump(xgb, 'my_models/models/cooling_demand_model_b'+str(i)+'_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        print(\"---------_-------------\")\n",
    "        lgb_ = LightGBM_Model(b,hyperparameter,'cooling_demand')\n",
    "        joblib.dump(lgb_,'my_models/models/LightGBM/cooling_demand_model_b'+str(i)+'_wo.h5')\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'cooling_demand')\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'cooling_demand')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/cooling_demand_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/cooling_demand_model_b'+str(i)+'.pkl')\n",
    "\n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Bi_Model(b,hyperparameter,'cooling_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_BiAttention/cooling_demand_model_b'+str(i)+'_hyper.h5')\n",
    "        \n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'cooling_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/cooling_demand_model_b'+str(i)+'_hyper.h5')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5d417-bec7-49b8-8f1a-2b30b199bbf1",
   "metadata": {},
   "source": [
    "### 2.) DHW Load (kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2291829-e34d-491d-a21f-9f6b8c981d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------_-------------\n",
      "29\n",
      "---------_-------------\n",
      "29\n",
      "---------_-------------\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "for b in b_dataframe_list:\n",
    "\n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'dhw_demand')\n",
    "        joblib.dump(xgb, 'my_models/models/dhw_demand_model_b'+str(i)+'_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        print(\"---------_-------------\")\n",
    "        lgb_ = LightGBM_Model(b,hyperparameter,'dhw_demand')\n",
    "        joblib.dump(lgb_, 'my_models/models/LightGBM/dhw_demand_model_b'+str(i)+'_wo.h5')\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'dhw_demand')\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'dhw_demand')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/dhw_demand_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/dhw_demand_model_b'+str(i)+'.pkl')\n",
    "        \n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Bi_Model(b,hyperparameter,'dhw_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_BiAttention/dhw_demand_model_b'+str(i)+'_hyper.h5')\n",
    "                        \n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'dhw_demand')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/dhw_demand_model_b'+str(i)+'_hyper.h5')\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f10a61-a65d-4a70-ad5f-01f02f0a2049",
   "metadata": {},
   "source": [
    "### 3.) Equipment Electric Power (kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4ecb764-a3e2-4266-a1ae-1da596ce021f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "29\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for b in b_dataframe_list:\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'non_shiftable_load')\n",
    "        joblib.dump(xgb, 'my_models/models/Equipment_Electric_Power_model_b'+str(i)+'_new_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        lgb_ = LightGBM_Model(b,hyperparameter,'non_shiftable_load')\n",
    "        joblib.dump(lgb_, 'my_models/models/LightGBM/Equipment_Electric_Power_model_b'+str(i)+'_wo.h5')\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'non_shiftable_load')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/Equipment_Electric_Power_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/Equipment_Electric_Power_model_b'+str(i)+'.pkl')\n",
    "        \n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Bi_Model(b,hyperparameter,'non_shiftable_load')\n",
    "        lstm_model.save('my_models/models/LSTM_BiAttention/Equipment_Electric_Power_model_b'+str(i)+'_hyper.h5')\n",
    "\n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'non_shiftable_load')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/Equipment_Electric_Power_model_b'+str(i)+'_hyper.h5')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5143b8-6eb1-424a-975f-434b42934d7c",
   "metadata": {},
   "source": [
    "# Neighbourhood Level Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6668b0-ba09-47d6-84f6-a4b392d12377",
   "metadata": {},
   "source": [
    "### 1.) Carbon Intensity (kgCO2e/kWh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "836e4946-9e37-4a24-83a7-9ea2b756110e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# combine the datasets to one since we only have one CI \n",
    "comb = pd.concat(b_dataframe_list, ignore_index=True)\n",
    "# Reset the index without adding an additional index column\n",
    "comb.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "if model_type == 'xgb':\n",
    "    xgb = XGBoost_Model(comb,hyperparameter,'carbon_intensity')\n",
    "    joblib.dump(xgb, 'my_models/models/Carbon_Intensity_Power_model_xgb.pkl')\n",
    "if model_type == 'lgb':\n",
    "    lgb_ = LightGBM_Model(comb,hyperparameter,'carbon_intensity')\n",
    "    joblib.dump(lgb_, 'my_models/models/LightGBM/Carbon_Intensity_Power_model_wo.h5')\n",
    "if model_type == 'fusion':\n",
    "    lgb_model  = LightGBM_Model(comb,hyperparameter,'carbon_intensity')\n",
    "    lstm_model = LSTM_Model(comb,hyperparameter,'carbon_intensity')\n",
    "        \n",
    "    joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/Carbon_Intensity_model.pkl')\n",
    "    joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/Carbon_Intensity_model.pkl')\n",
    "\n",
    "if model_type == 'lstm':\n",
    "    lstm_model = LSTM_Bi_Model(comb,hyperparameter,'carbon_intensity')\n",
    "    lstm_model.save('my_models/models/LSTM_BiAttention/Carbon_Intensity_model_hyper.h5')\n",
    "\n",
    "if model_type == 'bi-lstm':\n",
    "    lstm_model = LSTM_Bi_Model_hyper(comb,hyperparameter,'carbon_intensity')\n",
    "    lstm_model.save('my_models/models/LSTM_Bi_Model/Carbon_Intensity_model_hyper.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6048f-cd8c-4afb-ae16-da3e5e4bd5d8",
   "metadata": {},
   "source": [
    "### 2.) Solar Genetation (ConCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0146fac-ec4a-4bb8-a237-86bf867e2c27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000000\n",
      "1       0.000000\n",
      "2       0.000000\n",
      "3       0.000000\n",
      "4       0.000000\n",
      "          ...   \n",
      "2155    2.211529\n",
      "2156    0.000000\n",
      "2157    0.000000\n",
      "2158    0.000000\n",
      "2159    0.000000\n",
      "Name: solar_generation, Length: 2160, dtype: float32\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# combine the datasets to one since we only have one CI \n",
    "comb = pd.concat(b_dataframe_list, ignore_index=True)\n",
    "# Reset the index without adding an additional index column\n",
    "comb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "if model_type == 'xgb':\n",
    "    xgb = XGBoost_Model(comb,hyperparameter,'solar_generation')\n",
    "    joblib.dump(xgb, 'my_models/models/solar_generation_Power_model_xgb.pkl')\n",
    "if model_type == 'lgb':\n",
    "    lgb_ = LightGBM_Model(comb,hyperparameter,'solar_generation')\n",
    "    joblib.dump(lgb_, 'my_models/models/LightGBM/solar_generation_model_wo.h5')\n",
    "if model_type == 'fusion':\n",
    "    lgb_model  = LightGBM_Model(comb,hyperparameter,'solar_generation')\n",
    "    lstm_model = LSTM_Model(comb,hyperparameter,'carbon_intensity')\n",
    "        \n",
    "    joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/solar_generation_model.pkl')\n",
    "    joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/solar_generation_model_scaled.pkl')\n",
    "\n",
    "if model_type == 'lstm':\n",
    "    lstm_model = LSTM_Bi_Model(comb,hyperparameter,'solar_generation')\n",
    "    lstm_model.save('my_models/models/LSTM_BiAttention/solar_generation_model_hyper.h5')\n",
    "\n",
    "if model_type == 'bi-lstm':\n",
    "    lstm_model = LSTM_Bi_Model_hyper(comb,hyperparameter,'solar_generation')\n",
    "    lstm_model.save('my_models/models/LSTM_Bi_Model/solar_generation_model_hyper.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ede1eafd-c680-441c-85fe-40888b3f17f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.000000\n",
       "2       0.000000\n",
       "3       0.000000\n",
       "4       0.000000\n",
       "          ...   \n",
       "2155    2.211529\n",
       "2156    0.000000\n",
       "2157    0.000000\n",
       "2158    0.000000\n",
       "2159    0.000000\n",
       "Name: solar_generation, Length: 2160, dtype: float32"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb['solar_generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99ab8e-9d57-4554-96b5-6f69cc0041f3",
   "metadata": {},
   "source": [
    "### 2.) Solar Generation (W/kW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d128e16-9ebc-4318-864e-53f440826011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "for b in b_dataframe_list:\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        xgb = XGBoost_Model(b,hyperparameter,'solar_generation')\n",
    "        joblib.dump(xgb, 'my_models/models/solar_generation_model_b'+str(i)+'_xgb.pkl')\n",
    "    if model_type == 'lgb':\n",
    "        lgb = LightGBM_Model(b,hyperparameter,'solar_generation')\n",
    "        joblib.dump(lgb, 'my_models/models/solar_generation_model_b'+str(i)+'_lightgbm.pkl')\n",
    "    if model_type == 'fusion':\n",
    "        lgb_model  = LightGBM_Model(b,hyperparameter,'solar_generation')\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'solar_generation')\n",
    "        \n",
    "        joblib.dump(lgb_model, 'my_models/models/fusion/LightGBM/solar_generation_model_b'+str(i)+'.pkl')\n",
    "        joblib.dump(lstm_model, 'my_models/models/fusion/LSTM/solar_generation_model_b'+str(i)+'.pkl')\n",
    "    if model_type == 'lstm':\n",
    "        lstm_model = LSTM_Model(b,hyperparameter,'solar_generation')\n",
    "        lstm_model.save('my_models/models/LSTM/solar_generation_model_b'+str(i)+'.h5')\n",
    "\n",
    "    if model_type == 'bi-lstm':\n",
    "        lstm_model = LSTM_Bi_Model_hyper(b,hyperparameter,'solar_generation')\n",
    "        lstm_model.save('my_models/models/LSTM_Bi_Model/solar_generation_model_b'+str(i)+'_hyper.h5')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ab787-e32b-46d3-85ae-62dac941cd32",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e126c5a-4598-475a-a22b-001c1d1da927",
   "metadata": {},
   "source": [
    "### LightGBM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2dbe62f7-aa7c-4b50-88d1-7911ef439a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LightGBM_Model(b,hpt,feature):\n",
    "\n",
    "    # Load the feature selection\n",
    "    f_l = pd.read_csv('data/features/feature_importance_cooling_demand.csv')\n",
    "    \n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b#[f_l['feature']]\n",
    "    target   = b[feature]\n",
    "    print(str(target))\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if hpt == True:\n",
    "        params = {\n",
    "            'max_depth':        [3, 4, 5],\n",
    "            'num_leaves':       [10, 15, 20],\n",
    "            'learning_rate':    [0.05, 0.1, 0.15],\n",
    "            'n_estimators':     [50, 100, 200],\n",
    "            'subsample':        [0.5, 0.7, 0.9],\n",
    "            'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "            'reg_alpha':        [0.01, 0.1, 1],\n",
    "            'reg_lambda':       [0.01, 0.1, 1],\n",
    "            'steps':            [48],\n",
    "            'verbose':[-1]\n",
    "        }\n",
    "    \n",
    "        lgb_mean = LGBMRegressor(boosting_type='gbdt', objective='regression')\n",
    "        grid_search_mean = GridSearchCV(lgb_mean, params, cv=5, n_jobs=-1)\n",
    "        grid_search_mean.fit(X_train, y_train)\n",
    "        \n",
    "        # Create an AdaBoost model with LightGBM as the base estimator\n",
    "        #adaboost_model = AdaBoostRegressor(base_estimator=grid_search_mean, n_estimators=50)\n",
    "        #adaboost_model.fit(X_train, y_train)\n",
    "        \n",
    "        return grid_search_mean\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        lgb_params = {\n",
    "        'n_jobs': 1,\n",
    "        'max_depth': 4,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'subsample': 0.9,\n",
    "        'n_estimators': 80,\n",
    "        'learning_rate': 0.1,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'steps':48,\n",
    "        'verbose':-1,\n",
    "        }\n",
    "                \n",
    "        # fitting the model\n",
    "        gbm = LGBMRegressor(**lgb_params)\n",
    "        gbm.fit(X_train, y_train)\n",
    "        print(str(len(gbm.booster_.feature_name())))\n",
    "        # Create an AdaBoost model with LightGBM as the base estimator\n",
    "        #adaboost_model = AdaBoostRegressor(base_estimator=gbm, n_estimators=50)\n",
    "        #adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "        return gbm#adaboost_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a9ce8-9b05-444b-ad08-3e8b348dae11",
   "metadata": {},
   "source": [
    "### LightGBM Multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "279b92a0-7cef-4034-a775-3f0bb416684c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MultiStepLightGBM(b,hpt,feature):\n",
    "    \n",
    "    lgb_params = {\n",
    "        'n_jobs': 1,\n",
    "        'max_depth': 4,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'subsample': 0.9,\n",
    "        'n_estimators': 80,\n",
    "        'learning_rate': 0.1,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'verbose':-1,\n",
    "        }\n",
    "    \n",
    "\n",
    "    # Prepare lagged features for 48 steps ahead prediction\n",
    "    n_steps = 48\n",
    "    for i in range(1, n_steps + 1):\n",
    "        b[f'lag_{i}'] = b[feature].shift(-i)\n",
    "        \n",
    "    \n",
    "    X = b\n",
    "    y = b[feature]\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "        \n",
    "    # Create LightGBM dataset\n",
    "    train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test_scaled, label=y_test, reference=train_data)     \n",
    "        \n",
    "    \n",
    "    # Train the LightGBM model\n",
    "    num_round = 1000\n",
    "    bst = lgb.train(lgb_params, train_data, num_round, valid_sets=[test_data])\n",
    "    \n",
    "    return bst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f0258-fded-4949-89d0-abb445264942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_steps = 48\n",
    "for i in range(1, n_steps + 1):\n",
    "    print(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a4eb7-7354-489d-9be9-5b96f581b0f7",
   "metadata": {},
   "source": [
    "### CatBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c1069a8-60aa-44ec-9c7f-dbd6588ecb5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CatBoost_Model(b,hpt,feature):\n",
    "    \n",
    "    # Load the feature selection\n",
    "    f_l = pd.read_csv('data/features/feature_importance_'+str(feature)+'.csv')\n",
    "    \n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b#[f_l['feature']]\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    reg = XGBRegressor(n_estimators=100)\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            early_stopping_rounds=50)\n",
    "\n",
    "    return reg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0578c-a0d3-4710-85aa-10d0fe5b25f4",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d109d235-255f-43cf-84ef-bafddbc5d8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LSTM_Bi_Model_hyper(b, hyperparameter, feature):\n",
    "    \n",
    "    # Generate the x,y\n",
    "    X = b\n",
    "    y = b[feature]\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "    # Reshape data for LSTM input (samples, sequence_length, num_features)\n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    def objective(params, X_train, X_val, y_train, y_val):\n",
    "        units, learning_rate, dropout_input, dropout_lstm, num_layers, batch_size = params\n",
    "\n",
    "        # Reshape the input data to match the LSTM input shape\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_input, recurrent_dropout=dropout_lstm), input_shape=input_shape))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            model.add(Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_lstm, recurrent_dropout=dropout_lstm)))\n",
    "\n",
    "        model.add(LSTM(units=units, dropout=dropout_lstm, recurrent_dropout=dropout_lstm))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "        val_loss = model.evaluate(X_val, y_val)\n",
    "        return val_loss\n",
    "\n",
    "    space = [\n",
    "        (50, 150),                           # Number of LSTM units (units in each LSTM layer)\n",
    "        (1e-6, 1e-2, 'log-uniform'),         # Learning rate\n",
    "        (0.1, 0.9),                          # Dropout rate for input layer\n",
    "        (0.1, 0.9),                          # Dropout rate for LSTM layers\n",
    "        (1, 3),                              # Number of LSTM layers\n",
    "        (10, 100)                            # Batch size\n",
    "    ]\n",
    "\n",
    "    # Create a partial function without fixed arguments\n",
    "    objective_partial = partial(objective, X_train=X_train, X_val=X_test, y_train=y_train, y_val=y_test)\n",
    "\n",
    "    # Perform hyperparameter optimization\n",
    "    result = gp_minimize(objective_partial, space, n_calls=20, random_state=42)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_units, best_learning_rate, best_dropout_input, best_dropout_lstm, best_num_layers, best_batch_size = result.x\n",
    "    best_val_loss = result.fun\n",
    "\n",
    "    # Train the final model using the best hyperparameters\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_input, recurrent_dropout=best_dropout_lstm), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "    for _ in range(best_num_layers - 1):\n",
    "        best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm)))\n",
    "    \n",
    "    best_model.add(LSTM(units=best_units, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm))\n",
    "    best_model.add(Dense(1))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=best_learning_rate)\n",
    "    best_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    best_model.fit(X_train, y_train, epochs=1000, batch_size=best_batch_size, validation_data=(X_test, y_test), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ef75ce8-c244-4aed-a242-ce004b5d765a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "def LSTM_Bi_Model(b,hpt,feature):\n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    \n",
    "    # Reshape data for LSTM input (samples, sequence_length, num_features)\n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "\n",
    "    def objective_attention(params, X_train, X_val, y_train, y_val):\n",
    "        units, learning_rate, dropout_input, dropout_lstm, num_layers, batch_size = params\n",
    "\n",
    "        # Reshape the input data to match the LSTM input shape\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "        # Input layer\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        lstm_output = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_input, recurrent_dropout=dropout_lstm))(inputs)\n",
    "        for _ in range(num_layers - 1):\n",
    "            lstm_output = Bidirectional(LSTM(units=units, return_sequences=True, dropout=dropout_lstm, recurrent_dropout=dropout_lstm))(lstm_output)\n",
    "\n",
    "        # BahdanauAttention layer\n",
    "        context_vector, attention_weights = BahdanauAttention(units)(lstm_output, lstm_output)\n",
    "\n",
    "        # Concatenate context vector with LSTM output\n",
    "        lstm_output_combined = Concatenate()([context_vector, lstm_output])\n",
    "\n",
    "        # LSTM layer with return_sequences=False, go_backwards=True\n",
    "        lstm = LSTM(units=units, return_sequences=False, go_backwards=True)(lstm_output_combined)\n",
    "\n",
    "        # Output layer\n",
    "        output = Dense(48)(lstm)\n",
    "\n",
    "        # Create the model\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "        val_loss = model.evaluate(X_val, y_val)\n",
    "        return val_loss    \n",
    "    \n",
    "    \n",
    "    \n",
    "    space = [\n",
    "        (50, 150),                           # Number of LSTM units (units in each LSTM layer)\n",
    "        (1e-6, 1e-2, 'log-uniform'),         # Learning rate\n",
    "        (0.1, 0.9),                          # Dropout rate for input layer\n",
    "        (0.1, 0.9),                          # Dropout rate for LSTM layers\n",
    "        (1, 3),                              # Number of LSTM layers\n",
    "        (10, 100)                            # Batch size\n",
    "    ]\n",
    "\n",
    "    # Create a partial function without fixed arguments\n",
    "    objective_partial = partial(objective_attention, X_train=X_train, X_val=X_test, y_train=y_train, y_val=y_test)\n",
    "\n",
    "    # Perform hyperparameter optimization\n",
    "    result = gp_minimize(objective_partial, space, n_calls=20, random_state=42)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_units, best_learning_rate, best_dropout_input, best_dropout_lstm, best_num_layers, best_batch_size = result.x\n",
    "    best_val_loss = result.fun\n",
    "\n",
    "    # Train the final model using the best hyperparameters\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_input, recurrent_dropout=best_dropout_lstm), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "    for _ in range(best_num_layers - 1):\n",
    "        best_model.add(Bidirectional(LSTM(units=best_units, return_sequences=True, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm)))\n",
    "    \n",
    "    best_model.add(LSTM(units=best_units, dropout=best_dropout_lstm, recurrent_dropout=best_dropout_lstm))\n",
    "    best_model.add(Dense(48))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=best_learning_rate)\n",
    "    best_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    best_model.fit(X_train, y_train, epochs=1000, batch_size=best_batch_size, validation_data=(X_test, y_test), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    return best_model\n",
    "    \n",
    "    # BiDirectional LSTM with Attention:\n",
    "    #inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    #bidirectional_lstm = Bidirectional(LSTM(units=50, return_sequences=True))(inputs)\n",
    "    #bidirectional_lstm2 = Bidirectional(LSTM(units=50, return_sequences=True))(bidirectional_lstm)\n",
    "    #lstm, forward_h, forward_c, backward_h, backward_c = LSTM(units=50, return_state=True, go_backwards=True)(bidirectional_lstm2)\n",
    "    #state_h = Concatenate()([forward_h, backward_h])\n",
    "    #context_vector, attention_weights = BahdanauAttention(50)(lstm, lstm)\n",
    "\n",
    "    #output = Dense(units=48)(context_vector)\n",
    "    #model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Use early stopping to prevent overfitting\n",
    "    #early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    #model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping],verbose=0)\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd234c78-5aab-462a-8c7c-11c51c70f894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96430fdc-557d-4e7f-9995-3c6ad418da3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from keras.layers import Bidirectional\n",
    "\n",
    "def LSTM_Model(b,hpt,feature):\n",
    "\n",
    "    # Generate the x,y\n",
    "    features = b\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Reshape data for LSTM input (samples, sequence_length, num_features)\n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # LSTM with Attention: \n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    lstm = LSTM(units=50, return_sequences=True)(inputs)\n",
    "    lstm, forward_h, forward_c = LSTM(units=50, return_state=True, go_backwards=True)(lstm)\n",
    "    state_h = Concatenate()([forward_h, forward_c])\n",
    "    context_vector, attention_weights = BahdanauAttention(50)(lstm, lstm)\n",
    "\n",
    "    output = Dense(units=48)(context_vector)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "\n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping],verbose=0)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0169dbb8-7839-4245-984c-9f59b426562a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate\n",
    "import tensorflow as tf\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the custom layer here if necessary\n",
    "        super(BahdanauAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # Expand dimensions for broadcasting\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # Calculate context vector\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "        }\n",
    "        base_config = super(BahdanauAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e4371-f61f-4ea4-9a33-ed3ae5c55ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d2c58-43d7-4275-9b7f-759c4012cb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "024ea8fa-812f-4f1e-b0c0-ae8886e8c379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Fusion_Model(b,hpt,feature,lstm_model,lightgbm_model):\n",
    "    \n",
    "    # Load the feature selection\n",
    "    f_l = pd.read_csv('data/features/feature_importance_'+str(feature)+'.csv')\n",
    "    \n",
    "    \n",
    "    # Generate the x,y\n",
    "    features = b#[f_l['feature']]\n",
    "    target   = b[feature]\n",
    "\n",
    "    \n",
    "    # Generate the test,train \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "    X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # Create an ensemble model using VotingRegressor\n",
    "    ensemble_model = VotingRegressor(estimators=[('lstm', lstm_model), ('lgbm', lightgbm_model)])\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    return ensemble_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b38e0e-c673-41fe-8a97-b1890cc25c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6c123-ac24-4957-b317-74a0d5ec6731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ce45e-3ec4-4765-a8da-5568a4e5aabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f4244-379d-4021-bd9a-dd69ce80ecc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ba147-6d10-4b20-8cb5-56cb16f6cc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560235de-5577-4919-bdf6-1021ef6f79e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b85efd-8c1a-43bb-82a3-9d1ca6e195fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc2b52-6236-4670-89da-86023cbed3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cb36f-9613-48eb-90be-cffe73c682b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0fd6a-fc08-4de4-9682-9cc373d5a018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fa69f-e93f-427d-bee1-d415a68da321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725c757-8501-4b1e-b9cc-b9e71372a9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0a531-5522-4c3a-a99f-fb61b9d50d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cde1c7-050d-4667-9ba7-2e0a777a7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.datasets import load_longley\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "\n",
    "\n",
    "y, X = load_longley()\n",
    "y_train, y_test, X_train, X_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "fh = np.arange(len(X_train)) + 1\n",
    "\n",
    "\n",
    "forecaster = LGBMRegressor()\n",
    "forecaster.fit(y_train, X_train)\n",
    "y_pred = forecaster.predict(X=X_test)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856e569-7763-475e-b765-1c0250904f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e0caf-0083-443f-a21e-5eee2208859d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
